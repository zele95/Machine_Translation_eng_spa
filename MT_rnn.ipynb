{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5fqrooPhVyw",
        "outputId": "e41038b1-f214-4eda-8b2f-b0b307e86e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"You must do the work even if you don't like it.\", '[start] Tienes que hacer el trabajo aunque no te guste. [end]')\n",
            "(\"It'll just take a second.\", '[start] Solo tomará un segundo. [end]')\n",
            "('Most of the policemen lost their jobs.', '[start] La mayoría de los policías perdió su trabajo. [end]')\n",
            "(\"He is precisely the man we're looking for.\", '[start] Precisamente él es el hombre que buscamos. [end]')\n",
            "('I need a bag. Will you lend me one?', '[start] Necesito una bolsa. ¿Me puedes prestar una? [end]')\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# parse data\n",
        "with open('spa.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")[:-1]\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LsGs1cmhba1",
        "outputId": "599d8386-bda4-4de4-8307-1766131ad23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "138440 total pairs\n",
            "96908 training pairs\n",
            "20766 validation pairs\n",
            "20766 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aKrrNb9YhbfH"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV54wde0hbhR",
        "outputId": "cd113564-fd55-49cb-ecdd-d65a6ab1c958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "def format_dataset(eng, spa):\n",
        "  eng = eng_vectorization(eng)\n",
        "  spa = spa_vectorization(spa)\n",
        "  return ({\n",
        "    \"english\": eng,\n",
        "    \"spanish\": spa[:, :-1],\n",
        "  }, spa[:, 1:])  \n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHz7P--hbju",
        "outputId": "df4bae88-b517-41f1-97fc-beedb7451b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1515/1515 [==============================] - 143s 89ms/step - loss: 1.5928 - accuracy: 0.3973 - val_loss: 1.2764 - val_accuracy: 0.4864\n",
            "Epoch 2/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 1.1585 - accuracy: 0.5333 - val_loss: 1.0519 - val_accuracy: 0.5687\n",
            "Epoch 3/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.9602 - accuracy: 0.6065 - val_loss: 0.9599 - val_accuracy: 0.6115\n",
            "Epoch 4/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.8241 - accuracy: 0.6588 - val_loss: 0.9216 - val_accuracy: 0.6295\n",
            "Epoch 5/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.7222 - accuracy: 0.6986 - val_loss: 0.9153 - val_accuracy: 0.6354\n",
            "Epoch 6/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.6458 - accuracy: 0.7313 - val_loss: 0.9237 - val_accuracy: 0.6366\n",
            "Epoch 7/15\n",
            "1515/1515 [==============================] - 134s 88ms/step - loss: 0.5824 - accuracy: 0.7564 - val_loss: 0.9255 - val_accuracy: 0.6365\n",
            "Epoch 8/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.5335 - accuracy: 0.7764 - val_loss: 0.9326 - val_accuracy: 0.6356\n",
            "Epoch 9/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.4937 - accuracy: 0.7906 - val_loss: 0.9360 - val_accuracy: 0.6374\n",
            "Epoch 10/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.4587 - accuracy: 0.8032 - val_loss: 0.9419 - val_accuracy: 0.6353\n",
            "Epoch 11/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.4291 - accuracy: 0.8147 - val_loss: 0.9457 - val_accuracy: 0.6348\n",
            "Epoch 12/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.4034 - accuracy: 0.8235 - val_loss: 0.9585 - val_accuracy: 0.6335\n",
            "Epoch 13/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.3775 - accuracy: 0.8320 - val_loss: 0.9634 - val_accuracy: 0.6335\n",
            "Epoch 14/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.3536 - accuracy: 0.8380 - val_loss: 0.9704 - val_accuracy: 0.6330\n",
            "Epoch 15/15\n",
            "1515/1515 [==============================] - 133s 88ms/step - loss: 0.3304 - accuracy: 0.8461 - val_loss: 0.9739 - val_accuracy: 0.6313\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fafd80d2910>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "# source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "# x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "# encoded_source = layers.Bidirectional(\n",
        "# layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
        "\n",
        "# past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "# x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "# decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "# x = decoder_gru(x, initial_state=encoded_source)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "# seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
        "\n",
        "# Encoder training setup\n",
        "encoder_inputs = Input(shape=(None,),name = 'english')\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(x)\n",
        "encoder_states = [state_hidden, state_cell]\n",
        "\n",
        "# Decoder training setup:\n",
        "decoder_inputs = Input(shape=(None,),name = 'spanish')\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(x, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Building the training model:\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "# seq2seq_rnn.compile(\n",
        "# optimizer=\"rmsprop\",\n",
        "# loss=\"sparse_categorical_crossentropy\",\n",
        "# metrics=[\"accuracy\"])\n",
        "# seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)\n",
        "\n",
        "training_model.compile(\n",
        "optimizer=\"rmsprop\",\n",
        "loss=\"sparse_categorical_crossentropy\",\n",
        "metrics=[\"accuracy\"])\n",
        "training_model.fit(train_ds, epochs=15, validation_data=val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmaOjSa2hbll",
        "outputId": "76677c9f-46e5-4d5a-d6e4-1a3ccd62b8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-\n",
            "I'll call you as soon as I'm free.\n",
            "[start] te llamaré cuando quieras [end]\n",
            "-\n",
            "His jokes had us in stitches.\n",
            "[start] sus palabras nos [UNK] a la pegó en el correo [end]\n",
            "-\n",
            "Tom couldn't think of any reason for not giving Mary the present.\n",
            "[start] tom no podía pensar en ningún motivo para no [UNK] a mary [end]\n",
            "-\n",
            "A sponge absorbs water.\n",
            "[start] una frente está agua [end]\n",
            "-\n",
            "If anyone comes to see me, tell him that I am out.\n",
            "[start] si alguien llega mientras necesito estado mirando [end]\n",
            "-\n",
            "I'm getting married on Monday.\n",
            "[start] me estoy de lunes [end]\n",
            "-\n",
            "What are you talking about?\n",
            "[start] de qué estás hablando [end]\n",
            "-\n",
            "Have you lost weight?\n",
            "[start] has perdido peso [end]\n",
            "-\n",
            "He won the election by a large majority.\n",
            "[start] Él ganó la eso más grande de una gran [end]\n",
            "-\n",
            "There's your friend.\n",
            "[start] ahí es tu amigo [end]\n",
            "-\n",
            "It's improved.\n",
            "[start] está [UNK] [end]\n",
            "-\n",
            "I'm almost certain about that.\n",
            "[start] casi estoy de eso al respecto [end]\n",
            "-\n",
            "My spirit animal is a rabbit. What's yours?\n",
            "[start] mi propio precio es un nuevo eres cantante [end]\n",
            "-\n",
            "What have you come here for?\n",
            "[start] qué has aquí para estar aquí [end]\n",
            "-\n",
            "Tom grabbed the gun away from Mary before she could kill herself.\n",
            "[start] tom descubrió la pelota de maría antes de que lo querría con maría [end]\n",
            "-\n",
            "She ditched me.\n",
            "[start] ella me dejó [end]\n",
            "-\n",
            "The soup is terribly hot.\n",
            "[start] la sopa está tan caliente [end]\n",
            "-\n",
            "The settlers embraced the Christian religion.\n",
            "[start] los [UNK] [UNK] el acusado final de la revolución [end]\n",
            "-\n",
            "Tom could hear Mary, but he couldn't see her.\n",
            "[start] tom podía oír a mary pero ella no conocí [end]\n",
            "-\n",
            "I'm autistic.\n",
            "[start] soy [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = spa_vectorization([decoded_sentence])\n",
        "    # next_token_predictions = seq2seq_rnn.predict(\n",
        "    next_token_predictions = training_model.predict(\n",
        "    [tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSetuVypJSjj",
        "outputId": "9145676f-f55a-4294-8e5e-cf1229477887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[start] qué quiere comer [end]\n",
            "[start] cómo estuvo tu día [end]\n",
            "[start] voy a ir a la del nuevo amigo verdad [end]\n"
          ]
        }
      ],
      "source": [
        "print(decode_sequence('What do you want to eat?'))\n",
        "print(decode_sequence('How was your day?'))\n",
        "print(decode_sequence('I am going to the beast whith my best friend Ivan'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('DL_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d876f14a136bf9116447c888002c305d6a41514ae4dbe09b8bea86ee8a15820"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
